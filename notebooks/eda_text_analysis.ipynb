{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elle Evans (Joey King) has finally completed h...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A young girl tries to understand how she myste...</td>\n",
       "      <td>horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1800s England, a well meaning but selfish y...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abby Holland (Kristen Stewart) and Harper Cald...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Olga and Maks are 15 years apart. She is a suc...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description    genre\n",
       "0  Elle Evans (Joey King) has finally completed h...  romance\n",
       "1  A young girl tries to understand how she myste...   horror\n",
       "2  In 1800s England, a well meaning but selfish y...   comedy\n",
       "3  Abby Holland (Kristen Stewart) and Harper Cald...  romance\n",
       "4  Olga and Maks are 15 years apart. She is a suc...  romance"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../data/raw/IMDB_four_genre_larger_plot_description.csv')\n",
    "df.drop('movie_id',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Elle Evans (Joey King) has finally completed h...\n",
       "1      A young girl tries to understand how she myste...\n",
       "2      In 1800s England, a well meaning but selfish y...\n",
       "3      Abby Holland (Kristen Stewart) and Harper Cald...\n",
       "4      Olga and Maks are 15 years apart. She is a suc...\n",
       "                             ...                        \n",
       "995    In front of their little boy, Camille and Geor...\n",
       "996    After losing his wife and his memory in a car ...\n",
       "997    Based on the true-life experiences of Dave Fis...\n",
       "998    A troupe of hilariously self-obsessed theater ...\n",
       "999    A young mermaid makes a deal with a sea witch ...\n",
       "Name: description, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Description=df['description']\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\testr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\testr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\testr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\testr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNL=WordNetLemmatizer()\n",
    "def preprocess_text(text: str,return_lst=True) -> list:\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    text=text.lower()\n",
    "    tokens=word_tokenize(text)\n",
    "    lst=[]\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            token=WNL.lemmatize(token)\n",
    "            lst.append(token)\n",
    "    if return_lst:\n",
    "        return lst\n",
    "    else:\n",
    "        return ' '.join(lst)\n",
    "processed=Description.apply(preprocess_text,return_lst=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE=LabelEncoder()\n",
    "target=LE.fit_transform(df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(processed,target,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfidf=TfidfVectorizer()\n",
    "X_train_vector_sparse=Tfidf.fit_transform(X_train)\n",
    "X_test_vector_sparse=Tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation:\n",
      "Accuracy: 0.7100\n",
      "Precision: 0.7059\n",
      "Recall: 0.7100\n",
      "F1-Score: 0.7070\n",
      "Cross-val Accuracy: 0.7125 (±0.0319)\n",
      "\n",
      "Multinomial NB Evaluation:\n",
      "Accuracy: 0.6950\n",
      "Precision: 0.7275\n",
      "Recall: 0.6950\n",
      "F1-Score: 0.7060\n",
      "Cross-val Accuracy: 0.6825 (±0.0214)\n",
      "\n",
      "Random Forest Evaluation:\n",
      "Accuracy: 0.7200\n",
      "Precision: 0.7191\n",
      "Recall: 0.7200\n",
      "F1-Score: 0.7176\n",
      "Cross-val Accuracy: 0.6750 (±0.0256)\n",
      "\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: 0.4900\n",
      "Precision: 0.4930\n",
      "Recall: 0.4900\n",
      "F1-Score: 0.4724\n",
      "Cross-val Accuracy: 0.4650 (±0.0357)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "classifiers_tfidf = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers_tfidf.items():\n",
    "    print(f\"\\n{name} Evaluation:\")\n",
    "    \n",
    "    # Train model\n",
    "    clf.fit(X_train_vector_sparse, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_vector_sparse)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_vector_sparse, y_train, cv=skf, scoring='accuracy')\n",
    "    print(f\"Cross-val Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [text.split() for text in X_train]\n",
    "X_test_tokens = [text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=X_train_tokens,\n",
    "    window=5,\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def document_vector(doc_tokens: list, model: 'model') ->'embedding':\n",
    "    vectors = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_vectors_w2v = np.array([document_vector(doc, model_w2v) for doc in X_train_tokens])\n",
    "X_test_vectors_w2v = np.array([document_vector(doc, model_w2v) for doc in X_test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation:\n",
      "Accuracy: 0.5150\n",
      "Precision: 0.5214\n",
      "Recall: 0.5150\n",
      "F1-Score: 0.5053\n",
      "Cross-val Accuracy: 0.5150 (±0.0327)\n",
      "\n",
      "Gaussian NB Evaluation:\n",
      "Accuracy: 0.4900\n",
      "Precision: 0.5008\n",
      "Recall: 0.4900\n",
      "F1-Score: 0.4875\n",
      "Cross-val Accuracy: 0.5088 (±0.0400)\n",
      "\n",
      "Random Forest Evaluation:\n",
      "Accuracy: 0.5700\n",
      "Precision: 0.5848\n",
      "Recall: 0.5700\n",
      "F1-Score: 0.5755\n",
      "Cross-val Accuracy: 0.5712 (±0.0447)\n",
      "\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: 0.5200\n",
      "Precision: 0.5261\n",
      "Recall: 0.5200\n",
      "F1-Score: 0.5208\n",
      "Cross-val Accuracy: 0.4900 (±0.0429)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} Evaluation:\")\n",
    "    \n",
    "    # Train model\n",
    "    clf.fit(X_train_vectors_w2v, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_vectors_w2v)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_vectors_w2v, y_train, cv=skf, scoring='accuracy')\n",
    "    print(f\"Cross-val Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isnt capturing long term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_wick='With the price on his head ever increasing, legendary hit man John Wick takes his fight against the High Table global as he seeks out the most powerful players in the underworld, from New York to Paris to Japan to Berlin.'\n",
    "text=preprocess_text(john_wick,return_lst=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf=Tfidf.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers_tfidf['Logistic Regression'].predict(text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['action'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_w2v=document_vector(john_wick,model_w2v)\n",
    "classifiers['Logistic Regression'].predict([embedding_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['action'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Embeddings for better Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> MiniLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\testr\\Documents\\Projects\\Multi Modal Movie Genre Classification\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\testr\\Documents\\Projects\\Multi Modal Movie Genre Classification\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "X_train_vectors_transformer = model.encode(X_train.tolist()) \n",
    "X_test_vectors_transformer = model.encode(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Evaluation:\n",
      "Accuracy: 0.6950\n",
      "Precision: 0.6950\n",
      "Recall: 0.6950\n",
      "F1-Score: 0.6950\n",
      "Cross-val Accuracy: 0.7125 (±0.0401)\n",
      "\n",
      "Gaussian NB Evaluation:\n",
      "Accuracy: 0.6950\n",
      "Precision: 0.6932\n",
      "Recall: 0.6950\n",
      "F1-Score: 0.6925\n",
      "Cross-val Accuracy: 0.6887 (±0.0294)\n",
      "\n",
      "Random Forest Evaluation:\n",
      "Accuracy: 0.6550\n",
      "Precision: 0.6631\n",
      "Recall: 0.6550\n",
      "F1-Score: 0.6575\n",
      "Cross-val Accuracy: 0.6713 (±0.0184)\n",
      "\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: 0.3900\n",
      "Precision: 0.3813\n",
      "Recall: 0.3900\n",
      "F1-Score: 0.3804\n",
      "Cross-val Accuracy: 0.4600 (±0.0264)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers_transformer_MiniLM = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "for name, clf in classifiers_transformer_MiniLM.items():\n",
    "    print(f\"\\n{name} Evaluation:\")\n",
    "    \n",
    "    # Train model\n",
    "    clf.fit(X_train_vectors_transformer, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_vectors_transformer)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_vectors_transformer, y_train, cv=skf, scoring='accuracy')\n",
    "    print(f\"Cross-val Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> MPNET Base V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will require raw data for better understanding of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Description, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "X_train_vectors = model.encode(\n",
    "    X_train.tolist(),\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    truncate_dim=384\n",
    ")\n",
    "X_test_vectors = model.encode(\n",
    "    X_test.tolist(),\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    truncate_dim=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/processed/text/mpnet_enoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression Evaluation:\n",
      "Accuracy: 0.7350\n",
      "Precision: 0.7365\n",
      "Recall: 0.7350\n",
      "F1-Score: 0.7347\n",
      "Cross-val Accuracy: 0.7438 (±0.0119)\n",
      "\n",
      "Gaussian NB Evaluation:\n",
      "Accuracy: 0.7300\n",
      "Precision: 0.7274\n",
      "Recall: 0.7300\n",
      "F1-Score: 0.7283\n",
      "Cross-val Accuracy: 0.7125 (±0.0112)\n",
      "\n",
      "Random Forest Evaluation:\n",
      "Accuracy: 0.6900\n",
      "Precision: 0.6952\n",
      "Recall: 0.6900\n",
      "F1-Score: 0.6923\n",
      "Cross-val Accuracy: 0.6963 (±0.0135)\n",
      "\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: 0.5600\n",
      "Precision: 0.6054\n",
      "Recall: 0.5600\n",
      "F1-Score: 0.5670\n",
      "Cross-val Accuracy: 0.4813 (±0.0331)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifiers\n",
    "classifiers_transformer_mpnet = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "for name, clf in classifiers_transformer_mpnet.items():\n",
    "    print(f\"\\n{name} Evaluation:\")\n",
    "\n",
    "    # Train model\n",
    "    clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_vectors)\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_vectors, y_train, cv=skf, scoring='accuracy')\n",
    "    print(f\"Cross-val Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7500\n",
      "F1: 0.7567 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train_vectors = X_train_vectors.astype(np.float32)\n",
    "X_test_vectors = X_test_vectors.astype(np.float32)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),  # MPNet embeddings benefit from scaling\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        C=0.1,\n",
    "        class_weight='balanced',\n",
    "        solver='saga',\n",
    "        penalty='l2',\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train_vectors, y_train)\n",
    "y_pred = pipeline.predict(X_test_vectors)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying some neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ softmax (\u001b[38;5;33mSoftmax\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,948</span> (417.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,948\u001b[0m (417.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,948</span> (417.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,948\u001b[0m (417.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4174 - loss: 1.3310 - val_accuracy: 0.7188 - val_loss: 1.0730\n",
      "Epoch 2/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.6829 - loss: 0.9638 - val_accuracy: 0.7563 - val_loss: 0.7532\n",
      "Epoch 3/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.7745 - loss: 0.6451 - val_accuracy: 0.7375 - val_loss: 0.6939\n",
      "Epoch 4/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.7723 - loss: 0.5562 - val_accuracy: 0.7312 - val_loss: 0.6899\n",
      "Epoch 5/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.7940 - loss: 0.4975 - val_accuracy: 0.7437 - val_loss: 0.6823\n",
      "Epoch 6/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.8492 - loss: 0.4005 - val_accuracy: 0.7437 - val_loss: 0.7125\n",
      "Epoch 7/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.9133 - loss: 0.2758 - val_accuracy: 0.7375 - val_loss: 0.7566\n",
      "Epoch 8/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.9000 - loss: 0.2678 - val_accuracy: 0.6938 - val_loss: 0.8275\n",
      "Epoch 9/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.8951 - loss: 0.2800 - val_accuracy: 0.6938 - val_loss: 0.8030\n",
      "Epoch 10/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.9169 - loss: 0.2264 - val_accuracy: 0.7000 - val_loss: 0.8639\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.7172 - loss: 0.7808\n",
      "\n",
      "Test Accuracy: 0.7400\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        55\n",
      "           1       0.52      0.55      0.54        40\n",
      "           2       0.80      0.82      0.81        60\n",
      "           3       0.77      0.67      0.71        45\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.73      0.72      0.72       200\n",
      "weighted avg       0.74      0.74      0.74       200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "X_train = X_train_vectors.astype(np.float32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_test = X_test_vectors.astype(np.float32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# Model Architecture\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(768,))\n",
    "\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    x = layers.Dense(4)(x)\n",
    "    outputs = layers.Softmax(axis=-1)(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Model Configuration \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bidirectional LSTM to better maintain the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,088</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m49,664\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m33,088\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m516\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,036</span> (390.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m100,036\u001b[0m (390.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,036</span> (390.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m100,036\u001b[0m (390.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 412ms/step - accuracy: 0.2689 - loss: 1.3786 - val_accuracy: 0.4375 - val_loss: 1.3386\n",
      "Epoch 2/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 413ms/step - accuracy: 0.3883 - loss: 1.3423 - val_accuracy: 0.4812 - val_loss: 1.3079\n",
      "Epoch 3/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 413ms/step - accuracy: 0.4043 - loss: 1.3025 - val_accuracy: 0.4875 - val_loss: 1.2707\n",
      "Epoch 4/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 419ms/step - accuracy: 0.5353 - loss: 1.2625 - val_accuracy: 0.5063 - val_loss: 1.2328\n",
      "Epoch 5/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 415ms/step - accuracy: 0.5098 - loss: 1.2142 - val_accuracy: 0.5000 - val_loss: 1.1801\n",
      "Epoch 6/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 414ms/step - accuracy: 0.5146 - loss: 1.1762 - val_accuracy: 0.5188 - val_loss: 1.1278\n",
      "Epoch 7/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 419ms/step - accuracy: 0.5542 - loss: 1.1016 - val_accuracy: 0.5500 - val_loss: 1.0931\n",
      "Epoch 8/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 418ms/step - accuracy: 0.5950 - loss: 1.0660 - val_accuracy: 0.5250 - val_loss: 1.0600\n",
      "Epoch 9/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 421ms/step - accuracy: 0.5618 - loss: 1.0642 - val_accuracy: 0.5625 - val_loss: 1.0332\n",
      "Epoch 10/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 435ms/step - accuracy: 0.6171 - loss: 0.9754 - val_accuracy: 0.5750 - val_loss: 1.0175\n",
      "Epoch 11/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 422ms/step - accuracy: 0.6262 - loss: 0.9371 - val_accuracy: 0.5312 - val_loss: 1.0001\n",
      "Epoch 12/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 426ms/step - accuracy: 0.6386 - loss: 0.9299 - val_accuracy: 0.5500 - val_loss: 0.9818\n",
      "Epoch 13/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 421ms/step - accuracy: 0.6436 - loss: 0.8769 - val_accuracy: 0.5688 - val_loss: 0.9974\n",
      "Epoch 14/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 420ms/step - accuracy: 0.6014 - loss: 0.9245 - val_accuracy: 0.5688 - val_loss: 0.9772\n",
      "Epoch 15/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 422ms/step - accuracy: 0.6692 - loss: 0.8455 - val_accuracy: 0.5437 - val_loss: 0.9864\n",
      "Epoch 16/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 421ms/step - accuracy: 0.6558 - loss: 0.8513 - val_accuracy: 0.5375 - val_loss: 0.9877\n",
      "Epoch 17/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 423ms/step - accuracy: 0.6386 - loss: 0.8623 - val_accuracy: 0.5688 - val_loss: 0.9784\n",
      "Epoch 18/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 428ms/step - accuracy: 0.6899 - loss: 0.7778 - val_accuracy: 0.5688 - val_loss: 0.9902\n",
      "Epoch 19/50\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 17s 425ms/step - accuracy: 0.6560 - loss: 0.8472 - val_accuracy: 0.5500 - val_loss: 1.0076\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 184ms/step - accuracy: 0.6883 - loss: 0.7924\n",
      "\n",
      "Test Accuracy: 0.6950\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 166ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80        55\n",
      "           1       0.67      0.35      0.46        40\n",
      "           2       0.72      0.85      0.78        60\n",
      "           3       0.55      0.69      0.61        45\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.69      0.67      0.66       200\n",
      "weighted avg       0.70      0.69      0.68       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Model Architecture\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(768,))\n",
    "    # Reshape for sequential processing (24 timesteps x 32 features)\n",
    "    x = layers.Reshape((24, 32))(inputs)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Multi-head Attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Final layers\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Model Configuration\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combing both LogisticRegression with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training neural model...\n",
      "Epoch 1/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 11s 106ms/step - accuracy: 0.3084 - loss: 5.6603 - val_accuracy: 0.4625 - val_loss: 3.7622\n",
      "Epoch 2/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.4914 - loss: 3.3580 - val_accuracy: 0.6750 - val_loss: 2.4273\n",
      "Epoch 3/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.6771 - loss: 2.2349 - val_accuracy: 0.6938 - val_loss: 1.8065\n",
      "Epoch 4/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - accuracy: 0.7036 - loss: 1.7105 - val_accuracy: 0.7063 - val_loss: 1.5212\n",
      "Epoch 5/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.7810 - loss: 1.4568 - val_accuracy: 0.7063 - val_loss: 1.3799\n",
      "Epoch 6/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - accuracy: 0.7896 - loss: 1.3323 - val_accuracy: 0.7250 - val_loss: 1.2939\n",
      "Epoch 7/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.7570 - loss: 1.2281 - val_accuracy: 0.7125 - val_loss: 1.2265\n",
      "Epoch 8/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.7879 - loss: 1.1428 - val_accuracy: 0.7375 - val_loss: 1.1688\n",
      "Epoch 9/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 4s 88ms/step - accuracy: 0.8136 - loss: 1.0620 - val_accuracy: 0.7375 - val_loss: 1.1157\n",
      "Epoch 10/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.8091 - loss: 1.0100 - val_accuracy: 0.7188 - val_loss: 1.0751\n",
      "Epoch 11/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.8636 - loss: 0.9052 - val_accuracy: 0.7312 - val_loss: 1.0385\n",
      "Epoch 12/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 4s 88ms/step - accuracy: 0.8686 - loss: 0.8502 - val_accuracy: 0.7312 - val_loss: 1.0122\n",
      "Epoch 13/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.8641 - loss: 0.8109 - val_accuracy: 0.7750 - val_loss: 0.9853\n",
      "Epoch 14/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.8891 - loss: 0.7513 - val_accuracy: 0.7500 - val_loss: 0.9685\n",
      "Epoch 15/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.9000 - loss: 0.7149 - val_accuracy: 0.7750 - val_loss: 0.9517\n",
      "Epoch 16/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - accuracy: 0.9192 - loss: 0.6781 - val_accuracy: 0.7563 - val_loss: 0.9393\n",
      "Epoch 17/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.9120 - loss: 0.6593 - val_accuracy: 0.7688 - val_loss: 0.9257\n",
      "Epoch 18/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 4s 87ms/step - accuracy: 0.9302 - loss: 0.6450 - val_accuracy: 0.7625 - val_loss: 0.9191\n",
      "Epoch 19/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - accuracy: 0.9112 - loss: 0.6323 - val_accuracy: 0.7625 - val_loss: 0.9123\n",
      "Epoch 20/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - accuracy: 0.9530 - loss: 0.5738 - val_accuracy: 0.7688 - val_loss: 0.9023\n",
      "Epoch 21/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 88ms/step - accuracy: 0.9378 - loss: 0.5676 - val_accuracy: 0.7688 - val_loss: 0.8963\n",
      "Epoch 22/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - accuracy: 0.9459 - loss: 0.5367 - val_accuracy: 0.7688 - val_loss: 0.8897\n",
      "Epoch 23/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - accuracy: 0.9705 - loss: 0.5124 - val_accuracy: 0.7750 - val_loss: 0.8894\n",
      "Epoch 24/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 4s 92ms/step - accuracy: 0.9715 - loss: 0.4969 - val_accuracy: 0.7875 - val_loss: 0.8776\n",
      "Epoch 25/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.9684 - loss: 0.4742 - val_accuracy: 0.7625 - val_loss: 0.8777\n",
      "Epoch 26/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - accuracy: 0.9712 - loss: 0.4486 - val_accuracy: 0.7563 - val_loss: 0.8737\n",
      "Epoch 27/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 88ms/step - accuracy: 0.9696 - loss: 0.4624 - val_accuracy: 0.7812 - val_loss: 0.8639\n",
      "Epoch 28/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - accuracy: 0.9837 - loss: 0.4225 - val_accuracy: 0.7563 - val_loss: 0.8653\n",
      "Epoch 29/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 86ms/step - accuracy: 0.9943 - loss: 0.4063 - val_accuracy: 0.7812 - val_loss: 0.8528\n",
      "Epoch 30/30\n",
      "40/40 ━━━━━━━━━━━━━━━━━━━━ 5s 88ms/step - accuracy: 0.9892 - loss: 0.3992 - val_accuracy: 0.7875 - val_loss: 0.8464\n",
      "25/25 ━━━━━━━━━━━━━━━━━━━━ 1s 12ms/step\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 173ms/step\n",
      "\n",
      "Training stacked classifier...\n",
      "\n",
      "Final Stacked Accuracy: 0.7450\n",
      "7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 47ms/step\n",
      "Neural Model Accuracy: 0.7450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# 1. Feature Engineering Pipeline\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X_train_tfidf_dense = X_train_vector_sparse.toarray().astype(np.float32)\n",
    "X_test_tfidf_dense = X_test_vector_sparse.toarray().astype(np.float32)\n",
    "\n",
    "# Concatenate with MPNet embeddings\n",
    "X_train_combined = np.concatenate([X_train_vectors, X_train_tfidf_dense], axis=1)\n",
    "X_test_combined = np.concatenate([X_test_vectors, X_test_tfidf_dense], axis=1)\n",
    "\n",
    "# 2. Hybrid Model Architecture (No Flaky Layers)\n",
    "def create_robust_model(input_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Feature attention gate\n",
    "    attention = layers.Dense(input_dim, activation='sigmoid')(inputs)\n",
    "    x = layers.Multiply()([inputs, attention])\n",
    "\n",
    "    # Simple processing\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Initialize with combined feature dimension\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "model = create_robust_model(X_train_combined.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# 3. Stacked Training Approach\n",
    "\n",
    "\n",
    "# First train neural model\n",
    "print(\"Training neural model...\")\n",
    "model.fit(\n",
    "    X_train_combined,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Extract penultimate layer features\n",
    "feature_extractor = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[-2].output\n",
    ")\n",
    "train_features = feature_extractor.predict(X_train_combined)\n",
    "test_features = feature_extractor.predict(X_test_combined)\n",
    "\n",
    "# Final stacking with Logistic Regression\n",
    "print(\"\\nTraining stacked classifier...\")\n",
    "stacked_clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(C=0.1, max_iter=1000, class_weight='balanced')\n",
    ")\n",
    "stacked_clf.fit(train_features, y_train)\n",
    "\n",
    "# 4. Evaluation\n",
    "final_acc = stacked_clf.score(test_features, y_test)\n",
    "print(f\"\\nFinal Stacked Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# Compare with raw neural model\n",
    "nn_pred = model.predict(X_test_combined).argmax(1)\n",
    "nn_acc = np.mean(nn_pred == y_test)\n",
    "print(f\"Neural Model Accuracy: {nn_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41866"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.745\n",
      "precision_score: 0.7484458370170056\n",
      "recall_score: 0.745\n",
      "f1_score: 0.7452182461866595 \n"
     ]
    }
   ],
   "source": [
    "y_pred = stacked_clf.predict(test_features)\n",
    "\n",
    "print(\"accuracy_score:\",accuracy_score(y_test, y_pred))\n",
    "print(\"precision_score:\",precision_score(y_test, y_pred,average='weighted'))\n",
    "print(\"recall_score:\",recall_score(y_test, y_pred,average='weighted'))\n",
    "print(\"f1_score:\",f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented data for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: A sci-fi movie about space exploration with alien encounters\n",
      "Augmented: ['A movie about space with foreigner encounters', 'angstrom sci-fi movie space with']\n",
      "\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def simple_augment(text, num_augments=2):\n",
    "    \"\"\"Custom text augmentation using synonym replacement + random deletion\"\"\"\n",
    "    augmented = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    for _ in range(num_augments):\n",
    "        # Synonym replacement (40% of words)\n",
    "        mod_words = words.copy()\n",
    "        for i in range(len(mod_words)):\n",
    "            if random.random() < 0.4:\n",
    "                synonyms = wordnet.synsets(mod_words[i])\n",
    "                if synonyms:\n",
    "                    new_word = synonyms[0].lemmas()[0].name()\n",
    "                    mod_words[i] = new_word\n",
    "\n",
    "        # Random deletion (20% of words)\n",
    "        mod_words = [w for w in mod_words if random.random() > 0.2]\n",
    "\n",
    "        augmented.append(' '.join(mod_words))\n",
    "\n",
    "    return augmented\n",
    "\n",
    "original_text = \"A sci-fi movie about space exploration with alien encounters\"\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Augmented:\", simple_augment(original_text))\n",
    "\n",
    "all_augmented = []\n",
    "for desc in df['description']:\n",
    "    all_augmented.extend(simple_augment(desc))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_transformer = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "original_embeddings = model_transformer.encode(df['description'].tolist())\n",
    "augmented_embeddings = model_transformer.encode(all_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training size increased from 1000 to 3000\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 2s 4ms/step - loss: 1.1152 - accuracy: 0.5751 - val_loss: 0.8745 - val_accuracy: 0.7173\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.7801 - accuracy: 0.7093 - val_loss: 0.7289 - val_accuracy: 0.7360\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.7551 - val_loss: 0.6592 - val_accuracy: 0.7453\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5977 - accuracy: 0.7667 - val_loss: 0.6392 - val_accuracy: 0.7427\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5542 - accuracy: 0.7996 - val_loss: 0.6175 - val_accuracy: 0.7547\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.8164 - val_loss: 0.5976 - val_accuracy: 0.7560\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.5020 - accuracy: 0.8173 - val_loss: 0.5927 - val_accuracy: 0.7680\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4583 - accuracy: 0.8307 - val_loss: 0.5706 - val_accuracy: 0.7747\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.4393 - accuracy: 0.8396 - val_loss: 0.5780 - val_accuracy: 0.7840\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4335 - accuracy: 0.8413 - val_loss: 0.5658 - val_accuracy: 0.7813\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8500\n",
      "\n",
      "Final Test Accuracy: 0.8500 \n"
     ]
    }
   ],
   "source": [
    "X_train = np.vstack([original_embeddings, augmented_embeddings])\n",
    "y_train = np.concatenate([df['genre_encoded'],\n",
    "                        np.repeat(df['genre_encoded'], 2)])  # 2 augmentations per sample\n",
    "\n",
    "print(f\"Training size increased from {len(df)} to {len(X_train)}\")\n",
    "\n",
    "# Now train your model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(768,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        #tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_split=0.25,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test_vectors, y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91        55\n",
      "           1       0.68      0.80      0.74        40\n",
      "           2       0.86      0.90      0.88        60\n",
      "           3       0.84      0.82      0.83        45\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.84      0.84      0.84       200\n",
      "weighted avg       0.86      0.84      0.85       200 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improved but\n",
    "\n",
    "Precision of first class is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data for class: comedy (index 1)\n",
      "\n",
      "Original class distribution: [750 750 750 750]\n",
      "New class distribution: [ 750 1500  750  750]\n",
      "Added 750 samples for comedy \n"
     ]
    }
   ],
   "source": [
    "# Class Mapping\n",
    "CLASSES = {\n",
    "    0: \"action\", \n",
    "    1: \"comedy\",\n",
    "    2: \"drama\",\n",
    "    3: \"horror\"\n",
    "}\n",
    "\n",
    "# Reverse mapping for name->index\n",
    "CLASS_INDICES = {v:k for k,v in CLASSES.items()}\n",
    "\n",
    "\n",
    "problem_class_name = \"comedy\" \n",
    "class1_index = CLASS_INDICES[problem_class_name]\n",
    "\n",
    "# Verify\n",
    "print(f\"Augmenting data for class: {problem_class_name} (index {class1_index})\")\n",
    "\n",
    "# 2. Class-Specific Augmentation \n",
    "# Get original class counts\n",
    "original_counts = np.bincount(y_train)\n",
    "print(f\"\\nOriginal class distribution: {original_counts}\")\n",
    "\n",
    "# Filter DataFrame for problem class\n",
    "class1_mask = df['genre'] == problem_class_name\n",
    "class1_descs = df[class1_mask]['description'].tolist()\n",
    "\n",
    "# Generate 3x augmented versions\n",
    "augmented_class1 = []\n",
    "for text in class1_descs:  # 750+250*3\n",
    "    augmented = simple_augment(text, num_augments=3)  # Using our previous augmentation function\n",
    "    augmented_class1.extend(augmented)\n",
    "\n",
    "# Encode augmented texts\n",
    "X_train_class1_aug = model_transformer.encode(augmented_class1) \n",
    "y_train_class1_aug = np.full(len(augmented_class1), class1_index)\n",
    "\n",
    "# Combine datasets\n",
    "X_train_enhanced = np.vstack([X_train, X_train_class1_aug])\n",
    "y_train_enhanced = np.concatenate([y_train, y_train_class1_aug])\n",
    "\n",
    "# Verify new distribution\n",
    "new_counts = np.bincount(y_train_enhanced)\n",
    "print(f\"New class distribution: {new_counts}\")\n",
    "print(f\"Added {len(augmented_class1)} samples for {problem_class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 1.0962 - accuracy: 0.5630 - val_loss: 1.5012 - val_accuracy: 0.0773\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.7576 - accuracy: 0.6770 - val_loss: 1.3657 - val_accuracy: 0.2173\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.7110 - val_loss: 1.2876 - val_accuracy: 0.3227\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5900 - accuracy: 0.7510 - val_loss: 1.1848 - val_accuracy: 0.4147\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5543 - accuracy: 0.7670 - val_loss: 1.3276 - val_accuracy: 0.3640\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7863 - val_loss: 1.0052 - val_accuracy: 0.5440\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4867 - accuracy: 0.7997 - val_loss: 0.9535 - val_accuracy: 0.5467\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4664 - accuracy: 0.8147 - val_loss: 0.9594 - val_accuracy: 0.5600\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4424 - accuracy: 0.8207 - val_loss: 0.9481 - val_accuracy: 0.5760\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4187 - accuracy: 0.8297 - val_loss: 0.8126 - val_accuracy: 0.6387\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3957 - accuracy: 0.8380 - val_loss: 0.8043 - val_accuracy: 0.6480\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3763 - accuracy: 0.8457 - val_loss: 0.7631 - val_accuracy: 0.6653\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3673 - accuracy: 0.8523 - val_loss: 0.7618 - val_accuracy: 0.6693\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3517 - accuracy: 0.8530 - val_loss: 0.6921 - val_accuracy: 0.7080\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8650 - val_loss: 0.7628 - val_accuracy: 0.6600\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3166 - accuracy: 0.8730 - val_loss: 0.6790 - val_accuracy: 0.7013\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.8763 - val_loss: 0.7566 - val_accuracy: 0.6640\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2890 - accuracy: 0.8763 - val_loss: 0.7218 - val_accuracy: 0.6933\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2897 - accuracy: 0.8790 - val_loss: 0.5794 - val_accuracy: 0.7533\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2709 - accuracy: 0.8900 - val_loss: 0.6864 - val_accuracy: 0.7133\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2592 - accuracy: 0.8947 - val_loss: 0.5968 - val_accuracy: 0.7533\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2422 - accuracy: 0.9020 - val_loss: 0.6558 - val_accuracy: 0.7240\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.2336 - accuracy: 0.9013 - val_loss: 0.5122 - val_accuracy: 0.7853\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2245 - accuracy: 0.9127 - val_loss: 0.5025 - val_accuracy: 0.7907\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2186 - accuracy: 0.9067 - val_loss: 0.5941 - val_accuracy: 0.7467\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2095 - accuracy: 0.9137 - val_loss: 0.4910 - val_accuracy: 0.7973\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2019 - accuracy: 0.9130 - val_loss: 0.4835 - val_accuracy: 0.8000\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2037 - accuracy: 0.9140 - val_loss: 0.5592 - val_accuracy: 0.7773\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1898 - accuracy: 0.9213 - val_loss: 0.4358 - val_accuracy: 0.8213\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1863 - accuracy: 0.9170 - val_loss: 0.4616 - val_accuracy: 0.8013\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1798 - accuracy: 0.9230 - val_loss: 0.5195 - val_accuracy: 0.7920\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1705 - accuracy: 0.9290 - val_loss: 0.3749 - val_accuracy: 0.8453\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1770 - accuracy: 0.9223 - val_loss: 0.3787 - val_accuracy: 0.8347\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1646 - accuracy: 0.9353 - val_loss: 0.4168 - val_accuracy: 0.8253\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.1669 - accuracy: 0.9297 - val_loss: 0.4466 - val_accuracy: 0.8173\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1513 - accuracy: 0.9380 - val_loss: 0.4468 - val_accuracy: 0.8147\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1541 - accuracy: 0.9333 - val_loss: 0.3604 - val_accuracy: 0.8427\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1522 - accuracy: 0.9360 - val_loss: 0.3953 - val_accuracy: 0.8347\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1431 - accuracy: 0.9367 - val_loss: 0.4082 - val_accuracy: 0.8320\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1426 - accuracy: 0.9410 - val_loss: 0.3393 - val_accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1341 - accuracy: 0.9397 - val_loss: 0.3599 - val_accuracy: 0.8600\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1297 - accuracy: 0.9463 - val_loss: 0.3808 - val_accuracy: 0.8520\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1337 - accuracy: 0.9440 - val_loss: 0.3356 - val_accuracy: 0.8613\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1328 - accuracy: 0.9433 - val_loss: 0.3163 - val_accuracy: 0.8693\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1284 - accuracy: 0.9450 - val_loss: 0.3309 - val_accuracy: 0.8627\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1149 - accuracy: 0.9520 - val_loss: 0.4218 - val_accuracy: 0.8360\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1174 - accuracy: 0.9520 - val_loss: 0.3369 - val_accuracy: 0.8747\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1130 - accuracy: 0.9493 - val_loss: 0.3242 - val_accuracy: 0.8853\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1195 - accuracy: 0.9457 - val_loss: 0.3912 - val_accuracy: 0.8507\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1147 - accuracy: 0.9497 - val_loss: 0.3636 - val_accuracy: 0.8667 \n"
     ]
    }
   ],
   "source": [
    "# Training with Class Weights \n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount(y_train_enhanced)\n",
    "total_samples = len(y_train_enhanced)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "\n",
    "# Convert to dictionary format\n",
    "weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Build model\n",
    "model = create_model() \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with weights\n",
    "history = model.fit(\n",
    "    X_train_enhanced,\n",
    "    y_train_enhanced,\n",
    "    class_weight=weight_dict,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        55\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       1.00      1.00      1.00        60\n",
      "           3       1.00      1.00      1.00        45\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200 \n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_vectors)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, reducing the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 4s 5ms/step - loss: 1.0904 - accuracy: 0.5717 - val_loss: 1.4692 - val_accuracy: 0.1307\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7640 - accuracy: 0.6733 - val_loss: 1.3799 - val_accuracy: 0.2587\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.7340 - val_loss: 1.2909 - val_accuracy: 0.3587\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.5993 - accuracy: 0.7547 - val_loss: 1.2140 - val_accuracy: 0.4027\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.5523 - accuracy: 0.7800 - val_loss: 0.9853 - val_accuracy: 0.5413 \n"
     ]
    }
   ],
   "source": [
    "# Train lower epochs to avoid overfitting\n",
    "history = model.fit(\n",
    "    X_train_enhanced,\n",
    "    y_train_enhanced,\n",
    "    class_weight=weight_dict,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90        55\n",
      "           1       0.74      0.42      0.54        40\n",
      "           2       0.84      0.97      0.90        60\n",
      "           3       0.72      0.87      0.79        45\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.80      0.79      0.78       200\n",
      "weighted avg       0.81      0.81      0.80       200\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_vectors)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall for class 1 is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.7737 - accuracy: 0.7653 - val_loss: 0.3094 - val_accuracy: 0.9133\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7054 - accuracy: 0.7877 - val_loss: 0.3552 - val_accuracy: 0.8893\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.8033 - val_loss: 0.2942 - val_accuracy: 0.9080\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.8067 - val_loss: 0.3116 - val_accuracy: 0.9000\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5858 - accuracy: 0.8280 - val_loss: 0.3440 - val_accuracy: 0.8773 \n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:2.5, 2:1, 3:1} #Updated class weights\n",
    "history = model.fit(\n",
    "    X_train_enhanced,\n",
    "    y_train_enhanced,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        55\n",
      "           1       0.70      0.95      0.81        40\n",
      "           2       0.92      0.92      0.92        60\n",
      "           3       0.97      0.76      0.85        45\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.90      0.89      0.88       200\n",
      "weighted avg       0.91      0.89      0.89       200 \n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_vectors)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well balanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(LE, '../data/processed/text/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/processed/text/final_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
